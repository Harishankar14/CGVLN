import numpy as np
import networkx as nx

def detect_vulnerabilities(ir: nx.DiGraph, state):
    """
    Operator-level vulnerability detection (Section 2.4)
    Detects:
    - Adversarial susceptibility (Lipschitz constants)
    - Numerical instability (softmax, division, etc.)
    - Privacy leakage paths (skip connections, high sensitivity)
    """
    vulns = {
        'adversarial': [],
        'numerical': [],
        'privacy': []
    }
    
    # Track global Lipschitz constant
    global_lipschitz = 1.0
    layer_lipschitz = []
    
    try:
        topo_order = list(nx.topological_sort(ir))
    except:
        topo_order = list(ir.nodes())
    
    total_layers = len(topo_order)
    
    for idx, node in enumerate(topo_order):
        phi = ir.nodes[node].get('phi', {})
        psi = ir.nodes[node].get('psi', {})
        op_type = phi.get('type', '')
        
        # === ADVERSARIAL SUSCEPTIBILITY (Section 2.4) ===
        
        if op_type in ['Conv', 'Gemm', 'MatMul']:
            spectral_norm = psi.get('spectral_norm', 1.0)
            
            # Compute local Lipschitz constant
            if op_type == 'Conv':
                kh = phi.get('params', {}).get('kernel_h', 1)
                kw = phi.get('params', {}).get('kernel_w', 1)
                local_lipschitz = spectral_norm * np.sqrt(kh * kw)
            else:  # Fully connected
                local_lipschitz = spectral_norm
            
            global_lipschitz *= local_lipschitz
            layer_lipschitz.append((node, local_lipschitz))
            
            # Flag high local Lipschitz constants
            if local_lipschitz > 10.0:
                severity = 'critical' if local_lipschitz > 100 else 'high' if local_lipschitz > 50 else 'medium'
                vulns['adversarial'].append({
                    'node': node,
                    'type': 'high_lipschitz',
                    'severity': severity,
                    'lipschitz': float(local_lipschitz),
                    'message': f'Layer {node} has high Lipschitz constant ({local_lipschitz:.2f}), amplifying adversarial perturbations'
                })
        
        # Check for batch normalization reducing robustness (Section 1)
        if op_type == 'BatchNormalization':
            # Check if in final 3 layers
            layers_from_end = total_layers - idx
            if layers_from_end <= 3:
                vulns['adversarial'].append({
                    'node': node,
                    'type': 'bn_final_layers',
                    'severity': 'high',
                    'message': f'Batch normalization in final layers (position {idx}/{total_layers}) reduces robustness by 15-30%'
                })
        
        # === NUMERICAL INSTABILITY DETECTION (Section 2.4) ===
        
        # Pattern 1: Softmax without max-stabilization
        if op_type == 'Softmax':
            predecessors = list(ir.predecessors(node))
            has_stabilization = False
            
            for pred in predecessors:
                pred_type = ir.nodes[pred].get('phi', {}).get('type', '')
                if pred_type in ['ReduceMax', 'Max', 'Sub']:
                    has_stabilization = True
                    break
            
            if not has_stabilization:
                vulns['numerical'].append({
                    'node': node,
                    'type': 'unstable_softmax',
                    'severity': 'high',
                    'message': f'Softmax at {node} lacks max-subtraction stabilization, may overflow in FP16/INT8'
                })
        
        # Pattern 2: Division without epsilon term
        if op_type == 'Div':
            vulns['numerical'].append({
                'node': node,
                'type': 'potentially_unsafe_division',
                'severity': 'medium',
                'message': f'Division at {node} may lack epsilon term for numerical stability (verify denominator)'
            })
        
        # Pattern 3: BatchNorm with small epsilon
        if op_type == 'BatchNormalization':
            epsilon = phi.get('params', {}).get('epsilon', 1e-5)
            if epsilon < 1e-6:
                vulns['numerical'].append({
                    'node': node,
                    'type': 'small_bn_epsilon',
                    'severity': 'high',
                    'epsilon': float(epsilon),
                    'message': f'BatchNorm epsilon ({epsilon:.2e}) too small, may cause gradient explosion and reduce certified robustness by 15%'
                })
        
        # Pattern 4: Exp operations without bounds checking
        if op_type == 'Exp':
            # Check input bounds from robustness domain
            inp_min, inp_max = state.robustness.get(node, [0, 0])
            if inp_max > 88:  # exp(88) approaches float32 max
                vulns['numerical'].append({
                    'node': node,
                    'type': 'exp_overflow_risk',
                    'severity': 'critical',
                    'input_max': float(inp_max),
                    'message': f'Exp at {node} may overflow (input_max={inp_max:.2f}), needs input clipping'
                })
        
        # === PRIVACY LEAKAGE DETECTION (Section 2.4) ===
        
        # Pattern 1: Skip connections in final layers (Section 1)
        if op_type == 'Add':
            predecessors = list(ir.predecessors(node))
            
            if len(predecessors) >=2:
                 pred_indices = [topo_order.index(pred) for pred in predecessors if pred in topo_order]
                 if pred_indices and len(pred_indices) >=2:
                     skip_distance=max(pred_indices) - min(pred_indices)
                     
                     if skip_distance > 2:
                         in_final_layers = idx > 0.7 * total_layers
                         layers_from_end = total_layers - idx
                         near_output = layers_from_end <= 15
                         
                         if in_final_layers or near_output:
                             if skip_distance > 10 or layers_from_end <= 5:
                                 severity = 'high'
                             else:
                                 severity = 'medium'
                             vulns['privacy'].append({
                                 'node':node,
                                 'type':'skip_connection_leakage',
                                 'severity': severity,
                                 'skip_distance':skip_distance,
                                 'layers_from_end':layers_from_end,
                                 'message': f'Skip connection at {node} (distance={skip_distance}, {layers_from_end} layers from output) increases model inversion attack success by 3.2x'
                                 
                                 
                             })
                            
                            
        
        # Pattern 2: High condition number with taint propagation (Section 2.2)
        cond_num = psi.get('cond_num', 1.0)
        taint_score = state.privacy.get(node, 0.0)
        
        if cond_num > 1000 and taint_score > 0.5:
            vulns['privacy'].append({
                'node': node,
                'type': 'high_sensitivity_taint',
                'severity': 'medium',
                'cond_num': float(cond_num),
                'taint_score': float(taint_score),
                'message': f'Layer {node} has high condition number ({cond_num:.1f}) with tainted data flow, risk of training data leakage'
            })
        
        # Pattern 3: High gradient sensitivity (Section 2.2)
        # Approximate ||∂y/∂x|| using spectral norm chain
        if op_type in ['Conv', 'Gemm', 'MatMul']:
            spectral_norm = psi.get('spectral_norm', 1.0)
            if spectral_norm > 10 and taint_score > 0.3:  # τ = 10 for image classifiers
                vulns['privacy'].append({
                    'node': node,
                    'type': 'gradient_memorization_risk',
                    'severity': 'medium',
                    'sensitivity': float(spectral_norm),
                    'message': f'Layer {node} has high gradient sensitivity ({spectral_norm:.2f}), indicates memorization risk'
                })
    
    # Global adversarial vulnerability assessment
    if global_lipschitz > 1e6:
        vulns['adversarial'].append({
            'node': 'GLOBAL',
            'type': 'global_lipschitz',
            'severity': 'critical',
            'lipschitz': float(global_lipschitz),
            'message': f'Global Lipschitz constant {global_lipschitz:.2e} indicates extreme adversarial vulnerability'
        })
    elif global_lipschitz > 1e4:
        vulns['adversarial'].append({
            'node': 'GLOBAL',
            'type': 'global_lipschitz',
            'severity': 'high',
            'lipschitz': float(global_lipschitz),
            'message': f'Global Lipschitz constant {global_lipschitz:.2e} indicates high adversarial vulnerability'
        })
    
    # Gradient explosion detection (Section 2.4)
    if layer_lipschitz:
        product = 1.0
        for node, lip in layer_lipschitz:
            product *= lip
            if product > 1e6:
                vulns['numerical'].append({
                    'node': node,
                    'type': 'gradient_explosion',
                    'severity': 'critical',
                    'gradient_product': float(product),
                    'message': f'Gradient explosion detected: ∏||∂f/∂x|| = {product:.2e} > 10^6 at {node}'
                })
                break
    
    return vulns


def generate_report(vulnerabilities, output_file='vulnerability_report.txt'):
    """
    Generate human-readable vulnerability report with remediation guidance
    """
    with open(output_file, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("CGVulnScan - Neural Network Vulnerability Analysis Report\n")
        f.write("=" * 80 + "\n\n")
        
        # Summary
        total = sum(len(v) for v in vulnerabilities.values())
        f.write(f"Total Vulnerabilities Found: {total}\n")
        f.write(f"  - Adversarial: {len(vulnerabilities['adversarial'])}\n")
        f.write(f"  - Numerical: {len(vulnerabilities['numerical'])}\n")
        f.write(f"  - Privacy: {len(vulnerabilities['privacy'])}\n\n")
        
        # Adversarial vulnerabilities
        if vulnerabilities['adversarial']:
            f.write("\n" + "=" * 80 + "\n")
            f.write("ADVERSARIAL VULNERABILITIES\n")
            f.write("=" * 80 + "\n")
            for vuln in vulnerabilities['adversarial']:
                f.write(f"\n[{vuln['severity'].upper()}] {vuln['type']}\n")
                f.write(f"Node: {vuln['node']}\n")
                f.write(f"Message: {vuln['message']}\n")
                if 'lipschitz' in vuln:
                    f.write(f"Lipschitz Constant: {vuln['lipschitz']:.2e}\n")
                f.write("\nRemediation:\n")
                if vuln['type'] == 'high_lipschitz':
                    f.write("  - Apply spectral normalization to constrain weight matrices\n")
                    f.write("  - Add gradient clipping during training\n")
                elif vuln['type'] == 'bn_final_layers':
                    f.write("  - Remove batch normalization from final 3 layers\n")
                    f.write("  - Replace with layer normalization or group normalization\n")
        
        # Numerical vulnerabilities
        if vulnerabilities['numerical']:
            f.write("\n" + "=" * 80 + "\n")
            f.write("NUMERICAL INSTABILITY VULNERABILITIES\n")
            f.write("=" * 80 + "\n")
            for vuln in vulnerabilities['numerical']:
                f.write(f"\n[{vuln['severity'].upper()}] {vuln['type']}\n")
                f.write(f"Node: {vuln['node']}\n")
                f.write(f"Message: {vuln['message']}\n")
                f.write("\nRemediation:\n")
                if vuln['type'] == 'unstable_softmax':
                    f.write("  - Add max-subtraction: softmax(x - max(x))\n")
                    f.write("  - Use LogSoftmax + NLLLoss instead of Softmax + CrossEntropy\n")
                elif vuln['type'] == 'small_bn_epsilon':
                    f.write(f"  - Increase epsilon to 1e-5 or higher (current: {vuln.get('epsilon', 'unknown')})\n")
                elif vuln['type'] == 'exp_overflow_risk':
                    f.write("  - Clip inputs to exp() to [-88, 88] range\n")
        
        # Privacy vulnerabilities
        if vulnerabilities['privacy']:
            f.write("\n" + "=" * 80 + "\n")
            f.write("PRIVACY LEAKAGE VULNERABILITIES\n")
            f.write("=" * 80 + "\n")
            for vuln in vulnerabilities['privacy']:
                f.write(f"\n[{vuln['severity'].upper()}] {vuln['type']}\n")
                f.write(f"Node: {vuln['node']}\n")
                f.write(f"Message: {vuln['message']}\n")
                if 'cond_num' in vuln:
                    f.write(f"Condition Number: {vuln['cond_num']:.2e}\n")
                if 'taint_score' in vuln:
                    f.write(f"Taint Score: {vuln['taint_score']:.2f}\n")
                f.write("\nRemediation:\n")
                if vuln['type'] == 'skip_connection_leakage':
                    f.write("  - Remove or shorten skip connections in final 30% of network\n")
                    f.write("  - Apply differential privacy noise to gradients\n")
                elif vuln['type'] == 'high_sensitivity_taint':
                    f.write("  - Apply weight regularization to reduce condition number\n")
                    f.write("  - Consider DP-SGD training for privacy guarantees\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("End of Report\n")
        f.write("=" * 80 + "\n")
    
    print(f"Report saved to {output_file}")
